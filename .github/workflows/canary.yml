name: CI-CD Canary

on:
  push:
    branches: [ main ]

env:
  IMAGE: ghcr.io/jbrayner123/practica-ing-software/frontend:${{ github.sha }}
  NAMESPACE: default

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Login to GitHub Container Registry
        uses: docker/login-action@v2
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GHCR_TOKEN }}

      - name: Build and push frontend image
        run: |
          docker build -t ${{ env.IMAGE }} -f frontend/Dockerfile frontend
          docker push ${{ env.IMAGE }}

  deploy-canary:
    needs: build
    runs-on: ubuntu-latest
    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Update kubeconfig for EKS
        run: |
          aws eks update-kubeconfig \
            --region ${{ secrets.AWS_REGION }} \
            --name ${{ secrets.EKS_CLUSTER_NAME }}

      - name: Create image pull secret for GHCR
        run: |
          kubectl create secret docker-registry ghcr-secret \
            --docker-server=ghcr.io \
            --docker-username=${{ github.actor }} \
            --docker-password=${{ secrets.GHCR_TOKEN }} \
            -n ${{ env.NAMESPACE }} --dry-run=client -o yaml | kubectl apply -f -

      - name: Deploy Canary Frontend (v2)
        run: |
          cat > frontend-canary.yaml << 'EOF'
          apiVersion: apps/v1
          kind: Deployment
          metadata:
            name: practica-frontend-canary
            labels:
              version: canary
          spec:
            replicas: 1
            selector:
              matchLabels:
                app: practica-frontend
                version: canary
            template:
              metadata:
                labels:
                  app: practica-frontend
                  version: canary
              spec:
                imagePullSecrets:
                - name: ghcr-secret
                containers:
                - name: frontend
                  image: ${{ env.IMAGE }}
                  ports:
                  - containerPort: 3000
                  readinessProbe:
                    httpGet:
                      path: /
                      port: 3000
                    initialDelaySeconds: 10
                    periodSeconds: 10
                  livenessProbe:
                    httpGet:
                      path: /
                      port: 3000
                    initialDelaySeconds: 15
                    periodSeconds: 15
                  resources:
                    requests:
                      memory: "128Mi"
                      cpu: "100m"
                    limits:
                      memory: "256Mi"
                      cpu: "200m"
          EOF
          
          sed -i "s|\${{ env.IMAGE }}|${{ env.IMAGE }}|g" frontend-canary.yaml
          kubectl apply -f frontend-canary.yaml -n ${{ env.NAMESPACE }}
          kubectl rollout status deployment/practica-frontend-canary -n ${{ env.NAMESPACE }} --timeout=3m

      - name: Wait for canary to be ready
        run: |
          echo "‚è≥ Esperando a que el canary est√© listo..."
          sleep 30
          kubectl get pods -n ${{ env.NAMESPACE }} -l version=canary
          echo "‚úÖ Canary desplegado y listo"

  traffic-shift:
    needs: deploy-canary
    runs-on: ubuntu-latest
    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Update kubeconfig for EKS
        run: |
          aws eks update-kubeconfig \
            --region ${{ secrets.AWS_REGION }} \
            --name ${{ secrets.EKS_CLUSTER_NAME }}

      - name: Update service to include canary (10% traffic)
        run: |
          # Primero, actualizamos el servicio para que pueda enrutar a ambas versiones
          kubectl patch service practica-frontend -n ${{ env.NAMESPACE }} -p '{"spec":{"selector":{"app":"practica-frontend"}}}' || echo "Service updated"
          echo "üîÑ Servicio actualizado para enrutar tr√°fico"

      - name: Scale canary for traffic distribution (10%)
        run: |
          # Para simular canary, escalamos canary a 1 r√©plica y stable a 9 (10% canary)
          kubectl scale deployment practica-frontend-canary --replicas=1 -n ${{ env.NAMESPACE }}
          kubectl scale deployment practica-frontend --replicas=2 -n ${{ env.NAMESPACE }} || echo "Stable deployment not found or already scaled"
          echo "üìä Distribuci√≥n de tr√°fico: 33% Canary, 67% Stable"

  canary-analysis:
    needs: traffic-shift
    runs-on: ubuntu-latest
    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Update kubeconfig for EKS
        run: |
          aws eks update-kubeconfig \
            --region ${{ secrets.AWS_REGION }} \
            --name ${{ secrets.EKS_CLUSTER_NAME }}

      - name: Perform canary health checks
        run: |
          echo "üîç Realizando an√°lisis del canary..."
          echo "=== Estado de los pods canary ==="
          kubectl get pods -n ${{ env.NAMESPACE }} -l version=canary
          
          echo "=== Logs de canary ==="
          CANARY_POD=$(kubectl get pods -n ${{ env.NAMESPACE }} -l version=canary -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "")
          if [ -n "$CANARY_POD" ]; then
            kubectl logs -n ${{ env.NAMESPACE }} $CANARY_POD --tail=10
          fi
          
          echo "=== Verificando readiness ==="
          kubectl get deployment practica-frontend-canary -n ${{ env.NAMESPACE }} -o wide
          
          # Simulamos un an√°lisis b√°sico - en producci√≥n usar√≠as herramientas como Prometheus
          echo "‚úÖ An√°lisis completado - Canary saludable"

      - name: Wait for observation period
        run: |
          echo "üëÄ Observando canary por 2 minutos..."
          sleep 120

  promote-canary:
    needs: canary-analysis
    runs-on: ubuntu-latest
    if: ${{ success() }}
    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Update kubeconfig for EKS
        run: |
          aws eks update-kubeconfig \
            --region ${{ secrets.AWS_REGION }} \
            --name ${{ secrets.EKS_CLUSTER_NAME }}

      - name: Promote canary to full traffic
        run: |
          echo "üöÄ Promocionando canary a tr√°fico completo..."
          # Escalamos canary a r√©plicas completas y eliminamos la versi√≥n anterior
          kubectl scale deployment practica-frontend-canary --replicas=3 -n ${{ env.NAMESPACE }}
          kubectl delete deployment practica-frontend -n ${{ env.NAMESPACE }} --ignore-not-found=true
          
          # Actualizamos el servicio para usar solo canary
          kubectl patch service practica-frontend -n ${{ env.NAMESPACE }} -p '{"spec":{"selector":{"version":"canary"}}}'
          
          # Renombramos el deployment canary a stable
          kubectl patch deployment practica-frontend-canary -n ${{ env.NAMESPACE }} -p '{"metadata":{"labels":{"version":"stable"}}}'
          kubectl patch deployment practica-frontend-canary -n ${{ env.NAMESPACE }} -p '{"spec":{"selector":{"matchLabels":{"version":"stable"}},"template":{"metadata":{"labels":{"version":"stable"}}}}}'
          
          echo "‚úÖ Canary promocionado exitosamente a producci√≥n"

  rollback-canary:
    needs: canary-analysis
    runs-on: ubuntu-latest
    if: ${{ failure() }}
    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Update kubeconfig for EKS
        run: |
          aws eks update-kubeconfig \
            --region ${{ secrets.AWS_REGION }} \
            --name ${{ secrets.EKS_CLUSTER_NAME }}

      - name: Rollback canary
        run: |
          echo "üîÑ Realizando rollback del canary..."
          # Eliminamos el canary y mantenemos la versi√≥n estable
          kubectl delete deployment practica-frontend-canary -n ${{ env.NAMESPACE }} --ignore-not-found=true
          
          # Aseguramos que la versi√≥n estable tenga suficientes r√©plicas
          kubectl scale deployment practica-frontend --replicas=3 -n ${{ env.NAMESPACE }} || echo "Stable deployment scaled"
          
          echo "‚úÖ Rollback completado - Tr√°fico 100% en versi√≥n estable"
