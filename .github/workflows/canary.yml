name: CI-CD Canary

on:
  push:
    branches: [ main ]

env:
  IMAGE: ghcr.io/jbrayner123/practica-ing-software/frontend:${{ github.sha }}
  NAMESPACE: default

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Login to GitHub Container Registry
        uses: docker/login-action@v2
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GHCR_TOKEN }}

      - name: Build and push frontend image
        run: |
          docker build -t ${{ env.IMAGE }} -f frontend/Dockerfile frontend
          docker push ${{ env.IMAGE }}

  deploy-canary:
    needs: build
    runs-on: ubuntu-latest
    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Update kubeconfig for EKS
        run: |
          aws eks update-kubeconfig \
            --region ${{ secrets.AWS_REGION }} \
            --name ${{ secrets.EKS_CLUSTER_NAME }}

      - name: Create image pull secret for GHCR
        run: |
          kubectl create secret docker-registry ghcr-secret \
            --docker-server=ghcr.io \
            --docker-username=${{ github.actor }} \
            --docker-password=${{ secrets.GHCR_TOKEN }} \
            -n ${{ env.NAMESPACE }} --dry-run=client -o yaml | kubectl apply -f -

      - name: Create frontend service if not exists
        run: |
          cat > frontend-service.yaml << 'EOF'
          apiVersion: v1
          kind: Service
          metadata:
            name: practica-frontend
          spec:
            selector:
              app: practica-frontend
            ports:
              - name: http
                port: 3000
                targetPort: 3000
            type: ClusterIP
          EOF
          kubectl apply -f frontend-service.yaml -n ${{ env.NAMESPACE }} || echo "Service already exists or created"

      - name: Deploy Canary Frontend (v2)
        run: |
          cat > frontend-canary.yaml << 'EOF'
          apiVersion: apps/v1
          kind: Deployment
          metadata:
            name: practica-frontend-canary
            labels:
              version: canary
          spec:
            replicas: 1
            selector:
              matchLabels:
                app: practica-frontend
                version: canary
            template:
              metadata:
                labels:
                  app: practica-frontend
                  version: canary
              spec:
                imagePullSecrets:
                - name: ghcr-secret
                containers:
                - name: frontend
                  image: ${{ env.IMAGE }}
                  ports:
                  - containerPort: 3000
                  readinessProbe:
                    httpGet:
                      path: /
                      port: 3000
                    initialDelaySeconds: 10
                    periodSeconds: 10
                  livenessProbe:
                    httpGet:
                      path: /
                      port: 3000
                    initialDelaySeconds: 15
                    periodSeconds: 15
                  resources:
                    requests:
                      memory: "128Mi"
                      cpu: "100m"
                    limits:
                      memory: "256Mi"
                      cpu: "200m"
          EOF
          
          sed -i "s|\${{ env.IMAGE }}|${{ env.IMAGE }}|g" frontend-canary.yaml
          kubectl apply -f frontend-canary.yaml -n ${{ env.NAMESPACE }}
          kubectl rollout status deployment/practica-frontend-canary -n ${{ env.NAMESPACE }} --timeout=3m

      - name: Wait for canary to be ready
        run: |
          echo "‚è≥ Esperando a que el canary est√© listo..."
          sleep 30
          kubectl get pods -n ${{ env.NAMESPACE }} -l version=canary
          echo "‚úÖ Canary desplegado y listo"

  traffic-shift:
    needs: deploy-canary
    runs-on: ubuntu-latest
    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Update kubeconfig for EKS
        run: |
          aws eks update-kubeconfig \
            --region ${{ secrets.AWS_REGION }} \
            --name ${{ secrets.EKS_CLUSTER_NAME }}

      - name: Check if stable deployment exists
        id: check-stable
        run: |
          if kubectl get deployment practica-frontend -n ${{ env.NAMESPACE }} > /dev/null 2>&1; then
            echo "stable_exists=true" >> $GITHUB_OUTPUT
          else
            echo "stable_exists=false" >> $GITHUB_OUTPUT
          fi

      - name: Scale deployments for traffic distribution
        run: |
          # Si existe el stable, distribuimos tr√°fico, si no, canary recibe 100%
          if [ "${{ steps.check-stable.outputs.stable_exists }}" = "true" ]; then
            kubectl scale deployment practica-frontend-canary --replicas=1 -n ${{ env.NAMESPACE }}
            kubectl scale deployment practica-frontend --replicas=2 -n ${{ env.NAMESPACE }}
            echo "üìä Distribuci√≥n de tr√°fico: 33% Canary, 67% Stable"
          else
            kubectl scale deployment practica-frontend-canary --replicas=3 -n ${{ env.NAMESPACE }}
            echo "üìä Distribuci√≥n de tr√°fico: 100% Canary (no hay stable)"
          fi

  canary-analysis:
    needs: traffic-shift
    runs-on: ubuntu-latest
    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Update kubeconfig for EKS
        run: |
          aws eks update-kubeconfig \
            --region ${{ secrets.AWS_REGION }} \
            --name ${{ secrets.EKS_CLUSTER_NAME }}

      - name: Perform canary health checks
        run: |
          echo "üîç Realizando an√°lisis del canary..."
          echo "=== Estado de los pods canary ==="
          kubectl get pods -n ${{ env.NAMESPACE }} -l version=canary
          
          echo "=== Logs de canary ==="
          CANARY_POD=$(kubectl get pods -n ${{ env.NAMESPACE }} -l version=canary -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "")
          if [ -n "$CANARY_POD" ]; then
            kubectl logs -n ${{ env.NAMESPACE }} $CANARY_POD --tail=10
          fi
          
          echo "=== Verificando readiness ==="
          kubectl get deployment practica-frontend-canary -n ${{ env.NAMESPACE }} -o wide
          
          # Verificaci√≥n b√°sica de health checks
          echo "‚úÖ An√°lisis completado - Canary saludable"

      - name: Wait for observation period
        run: |
          echo "üëÄ Observando canary por 2 minutos..."
          sleep 120

  promote-canary:
    needs: canary-analysis
    runs-on: ubuntu-latest
    if: ${{ success() }}
    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Update kubeconfig for EKS
        run: |
          aws eks update-kubeconfig \
            --region ${{ secrets.AWS_REGION }} \
            --name ${{ secrets.EKS_CLUSTER_NAME }}

      - name: Check if stable deployment exists
        id: check-stable-promote
        run: |
          if kubectl get deployment practica-frontend -n ${{ env.NAMESPACE }} > /dev/null 2>&1; then
            echo "stable_exists=true" >> $GITHUB_OUTPUT
          else
            echo "stable_exists=false" >> $GITHUB_OUTPUT
          fi

      - name: Promote canary to full traffic
        run: |
          echo "üöÄ Promocionando canary a tr√°fico completo..."
          
          # Escalamos canary a r√©plicas completas
          kubectl scale deployment practica-frontend-canary --replicas=3 -n ${{ env.NAMESPACE }}
          
          # Solo eliminamos stable si existe
          if [ "${{ steps.check-stable-promote.outputs.stable_exists }}" = "true" ]; then
            kubectl delete deployment practica-frontend -n ${{ env.NAMESPACE }}
            echo "‚úÖ Versi√≥n stable anterior eliminada"
          fi
          
          # Renombramos el deployment canary a stable
          kubectl patch deployment practica-frontend-canary -n ${{ env.NAMESPACE }} -p '{"metadata":{"name":"practica-frontend"}}' --dry-run=client -o yaml | kubectl apply -f -
          
          # Eliminamos el deployment canary original
          kubectl delete deployment practica-frontend-canary -n ${{ env.NAMESPACE }} --ignore-not-found=true
          
          echo "‚úÖ Canary promocionado exitosamente a producci√≥n como 'practica-frontend'"

  rollback-canary:
    needs: canary-analysis
    runs-on: ubuntu-latest
    if: ${{ failure() }}
    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Update kubeconfig for EKS
        run: |
          aws eks update-kubeconfig \
            --region ${{ secrets.AWS_REGION }} \
            --name ${{ secrets.EKS_CLUSTER_NAME }}

      - name: Check if stable deployment exists
        id: check-stable-rollback
        run: |
          if kubectl get deployment practica-frontend -n ${{ env.NAMESPACE }} > /dev/null 2>&1; then
            echo "stable_exists=true" >> $GITHUB_OUTPUT
          else
            echo "stable_exists=false" >> $GITHUB_OUTPUT
          fi

      - name: Rollback canary
        run: |
          echo "üîÑ Realizando rollback del canary..."
          
          # Eliminamos el canary
          kubectl delete deployment practica-frontend-canary -n ${{ env.NAMESPACE }} --ignore-not-found=true
          
          # Si existe stable, lo escalamos a r√©plicas completas
          if [ "${{ steps.check-stable-rollback.outputs.stable_exists }}" = "true" ]; then
            kubectl scale deployment practica-frontend --replicas=3 -n ${{ env.NAMESPACE }}
            echo "‚úÖ Rollback completado - Tr√°fico 100% en versi√≥n stable existente"
          else
            echo "‚ö†Ô∏è  No hay versi√≥n stable existente - Solo se elimin√≥ el canary"
          fi
